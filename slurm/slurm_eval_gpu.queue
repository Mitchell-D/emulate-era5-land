#!/bin/csh
### SLURM batch script

### Email address
#SBATCH --mail-user=mtd0012@uah.edu

### queue type
#SBATCH -p standard --gres=gpu:1

### TOTAL processors (number of tasks)
#SBATCH --ntasks 13
####SBATCH --ntasks 1

### total run time estimate (D-HH:MM)
#SBATCH -t 5-00:00

### memory (MB per CPU)
#SBATCH --mem-per-cpu=12G
###SBATCH --mem-per-cpu=24G
###SBATCH --mem-per-cpu=48G

### Mail to user on job done and fail
#SBATCH --mail-type=END,FAIL

### Job name
###SBATCH -J gen-test
#SBATCH -J lstm-eval

###SBATCH --open-mode=append ## Don't overwrite existing files
###SBATCH -o /rhome/mdodson/emulate-era5-land/slurm/out/slurm_gridstats_eval.out
###SBATCH -e /rhome/mdodson/emulate-era5-land/slurm/out/slurm_gridstats_eval.err
###SBATCH -o /rhome/mdodson/emulate-era5-land/slurm/out/slurm_test_gens.out
###SBATCH -e /rhome/mdodson/emulate-era5-land/slurm/out/slurm_test_gens.err
#SBATCH -o /rhome/mdodson/emulate-era5-land/slurm/out/slurm_eval_models_acclstm-era5-swm-64.out
#SBATCH -e /rhome/mdodson/emulate-era5-land/slurm/out/slurm_eval_models_acclstm-era5-swm-64.err

##module load cuda/11.8
module load cuda/12.4

## Run code
set runcmd =  /rhome/mdodson/.micromamba/envs/learn-torch-meteor/bin/python

### Set dynamic link loader path variable to include CUDA and bins from mamba
#setenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:${mamba_env_path}/lib
setenv LD_LIBRARY_PATH ${runcmd}/lib

echo $LD_LIBRARY_PATH
nvidia-smi

cd /rhome/mdodson/emulate-era5-land

#${runcmd} -u scripts/test_generators.py
##${runcmd} -u scripts/eval_gridstats.py
${runcmd} -u scripts/eval_models.py
